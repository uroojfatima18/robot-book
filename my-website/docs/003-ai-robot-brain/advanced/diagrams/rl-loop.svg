<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 400" role="img" aria-labelledby="title desc">
  <title id="title">Reinforcement Learning Training Loop</title>
  <desc id="desc">Circular diagram showing the reinforcement learning loop. The Agent (orange, contains Policy neural network) observes State from the Environment (blue, represents simulation or real world). Based on the state, the agent selects an Action. The environment processes the action, returns a Reward signal (green), and transitions to a new State. The agent uses rewards to update its policy to maximize cumulative reward over time.</desc>

  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#000000"/>
    </marker>
  </defs>

  <style>
    .box { stroke: #000000; stroke-width: 2; }
    .title { font-family: sans-serif; font-size: 16px; font-weight: bold; fill: #000000; text-anchor: middle; }
    .subtitle { font-family: sans-serif; font-size: 12px; fill: #000000; text-anchor: middle; }
    .label { font-family: sans-serif; font-size: 11px; fill: #000000; text-anchor: middle; }
    .arrow { stroke: #000000; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
    .agent { fill: #E69F00; }
    .env { fill: #56B4E9; }
    .reward { fill: #009E73; }
  </style>

  <!-- Agent -->
  <rect x="50" y="100" width="200" height="150" rx="15" class="box agent"/>
  <text x="150" y="135" class="title">Agent</text>

  <!-- Policy inside agent -->
  <rect x="75" y="155" width="150" height="70" rx="10" fill="#FFFFFF" stroke="#000" stroke-width="1"/>
  <text x="150" y="180" class="subtitle">Policy π(a|s)</text>
  <text x="150" y="200" class="subtitle" font-size="10">Neural Network</text>

  <!-- Environment -->
  <rect x="450" y="100" width="200" height="150" rx="15" class="box env"/>
  <text x="550" y="135" class="title">Environment</text>
  <text x="550" y="165" class="subtitle">Simulation or</text>
  <text x="550" y="185" class="subtitle">Real World</text>
  <!-- Robot icon -->
  <rect x="520" y="200" width="60" height="30" rx="5" fill="#FFFFFF" stroke="#000"/>
  <circle cx="535" cy="230" r="8" fill="#333"/>
  <circle cx="565" cy="230" r="8" fill="#333"/>

  <!-- State arrow (Environment -> Agent) -->
  <path d="M 450 140 Q 350 50 250 140" class="arrow"/>
  <rect x="310" y="60" width="80" height="30" rx="5" fill="#56B4E9"/>
  <text x="350" y="80" class="label" fill="#FFFFFF">State s</text>

  <!-- Action arrow (Agent -> Environment) -->
  <path d="M 250 200 Q 350 200 450 200" class="arrow"/>
  <rect x="310" y="180" width="80" height="30" rx="5" fill="#E69F00"/>
  <text x="350" y="200" class="label">Action a</text>

  <!-- Reward arrow (Environment -> Agent, bottom) -->
  <path d="M 450 230 Q 350 330 250 230" class="arrow"/>
  <rect x="305" y="290" width="90" height="30" rx="5" fill="#009E73"/>
  <text x="350" y="310" class="label" fill="#FFFFFF">Reward r</text>

  <!-- Learning update annotation -->
  <text x="150" y="280" class="subtitle">Update policy</text>
  <text x="150" y="295" class="subtitle">to maximize</text>
  <text x="150" y="310" class="subtitle">cumulative reward</text>

  <!-- Goal annotation -->
  <rect x="500" y="320" width="180" height="60" rx="5" fill="#FFFFFF" stroke="#000" stroke-dasharray="5,5"/>
  <text x="590" y="345" class="subtitle">Goal: Learn π* that</text>
  <text x="590" y="365" class="subtitle">maximizes E[Σγᵗrₜ]</text>

  <!-- Time step annotation -->
  <text x="350" y="385" class="label" font-style="italic">Repeat for many timesteps</text>
</svg>
